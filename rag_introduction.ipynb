{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Retrieval Augmented Generation\n",
    "\n",
    "This notebook introduces how to work with Langchain. \n",
    "Made by Csaba Hegedűs, BME-TMIT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 0 Setup\n",
    "\n",
    "Same as the Langchain intro. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python packages \n",
    "Installing prerequisites: langchain and langgraph libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet langchain langchain-community langchain-openai langchain_chroma "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure LLM\n",
    "\n",
    "Always run this, before trying out anything else. \n",
    "\n",
    "You can use OpenAI or AzureOpenAI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_ENDPOINT = \"\"\n",
    "AZURE_OPENAI_API_KEY = \"\"\n",
    "AZURE_OPENAI_API_VERSION = \"2024-05-01-preview\"\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = \"gpt4o\"\n",
    "AZURE_OPENAI_EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    deployment_name=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    ")\n",
    "\n",
    "embedder = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    model=AZURE_OPENAI_EMBEDDING_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALTERNATIVE: Using OpenAI as LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o\")\n",
    "embedder = OpenAIEmbeddings(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1 Processing documents \n",
    "\n",
    "Document load, split, store (embed). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to load the document into a Document object.\n",
    "Follow this tutorial, if need additional help: https://python.langchain.com/v0.2/docs/tutorials/rag/\n",
    "\n",
    " There are many types of loaders in Langchain: https://python.langchain.com/v0.2/docs/integrations/document_loaders/\n",
    "\n",
    " How to load PDFs specifically: https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/\n",
    "\n",
    " I have used Unstructured library, because it has built in OCR, supports multi-modality and many file types. Has Langchain integration: https://python.langchain.com/v0.2/docs/integrations/providers/unstructured/\n",
    "\n",
    " However, for an introduction demo, it is sufficient to use a simplier loader. So we will use the PyPDF loader: https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/#using-pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pypdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Co-pilots for Arrowhead-based\\nCyber-Physical System of Systems Engineering\\nCsaba Heged ˝us, P ´al Varga\\nDepartment of Telecommunications and Artificial Intelligence\\nBudapest University of Technology and Economics\\nM˝uegyetem rkp. 3., H-1111 Budapest, Hungary.\\nEmail: {hegeduscs, pvarga }@tmit.bme.hu\\nAbstract —One benefit of Large Language Model (LLM) based\\napplications (e.g. chat assistants or co-pilots) is that they can\\nbring humans closer to the loop in various IT and OT solutions.\\nCo-pilots can achieve many things at once, i.e. provide a context-\\naware natural language interface to knowledge bases, reach\\nvarious systems (via APIs), or even help solving multi-step\\nproblems with their planning and reasoning abilities. However,\\nmaking production-grade chat assistants is a topical challenge,\\nas fast-evolving LLMs expose new types of application design\\nand security issues that need tackling. These especially rise\\nto power when we try to apply these solutions to industrial\\nautomation use cases, as they need additional explainability and\\nreliability engineered into the architecture. This paper describes\\nthe envisioned use cases and the findings of proof of concept\\ncopilots for the Cyber-Physical System of Systems (CPSoS)\\nengineering domain. The paper suggests three types of copilots to\\nsupport the stages throughout the CPSoS engineering lifecycle –\\nand shows Proof-of-Concept scenarios for the Eclipse Arrowhead\\nengineering process.\\nI. I NTRODUCTION\\nThe year 2023 has mostly been spent around generative\\nArtificial Intelligence (genAI) and Large Language Models\\n(LLM). Various commercial products and open-source projects\\nhave appeared and are showing great evolution in just a\\nvery short timeframe. Most notable ones include the products\\nof OpenAI and Microsoft, ChatGPT 4 and its text-to-image\\nmodel Dall-E [1].\\nNow, every industry and domain is looking at how to\\nutilize these technologies in their business and keep up with\\ntheir development speed in the productization phases. Building\\nsolutions with these products capable of semantic reasoning\\nand contextual content generation, among others, enables a lot\\nof new ideas to come forth from product owners [2]. Many\\ncompanies, Microsoft for example, are building their LLM-\\nand genAI-based product portfolio around the concept of co-\\npilots .\\nCo-pilots, or chat assistants, refer to an AI-based solution\\naiding users in various customer journeys of an ecosystem.\\nFor example, the Microsoft Copilot [3] suite is a horizontal\\nsolution in the Microsoft 365 and Azure ecosystems that has\\naccess to the user’s (and organization’s) data in Azure and\\nEntra ID , and can interact with various Microsoft 365 (Office,\\nPower and Azure platform) applications via Application Pro-\\ngramming Interfaces (APIs) and specialized Domain SpecificLanguages (DSLs) to generate, summarise or augment (multi-\\nmodal) content; or act on behalf of the user within these\\napplications.\\nNevertheless, rightful concerns have arisen around the\\nsafety, reliability and the responsible use of all AI-based\\nsolutions. Explainable AI (XAI) emphasizes transparent model\\narchitectures and feature importance rankings, as it is essential\\nto comprehend the reasoning behind the AI’s decisions and\\nactions. This is especially the case for potential industrial\\n(manufacturing) applications, as ours. Our chat assistant needs\\nto be able to provide plans and explanations that consider the\\nspecific industrial domain knowledge, so its users can trust\\nand validate the outcome [4].\\nA. Capabilities of Chat Assistants\\nBased on the current state of the business, these chat-based\\nco-pilot agents can have many interesting features. Primarily,\\nthe capabilities depicted in Table I are considered for this\\npaper. Besides these, there is one additional capability worth\\nmentioning: the code interpreter sandbox, where the model can\\nrun generated code snippets against uploaded files. However,\\nthis feature is out of the scope of this paper.', metadata={'source': './copilotRC.pdf', 'page': 0}),\n",
      " Document(page_content='mentioning: the code interpreter sandbox, where the model can\\nrun generated code snippets against uploaded files. However,\\nthis feature is out of the scope of this paper.\\nMost of these functionalities can be developed using mod-\\nern, cloud-native application design patterns and LLM applica-\\ntion orchestration platforms, such as LangChain [5], llamain-\\ndex [6], Haystack [7] or Semantic Kernel and Memory (from\\nMicrosoft) [8].\\nB. Motivation and Structure of the Paper\\nThe purpose of this paper is to demonstrate that LLM-based\\nco-pilots can be implemented for industrial automation as\\nwell, to aid the development and operations of large-scale Sys-\\ntem of Systems (SoS). We will also report on the qualitative\\nfindings around a proof-of-concept (PoC) demonstrator that\\nintegrates into the framework and showcases the values added\\nacross the Arrowhead toolchain.\\nThis work is to be considered in the context of the Eclipse\\nArrowhead framework [9], which is a Service Oriented Archi-\\ntecture (SOA) driven design framework and middleware im-\\nplementation to govern and orchestrate Cyber-Physical System\\nof systems (CPSoS). Figure 1 depicts the deployment structure', metadata={'source': './copilotRC.pdf', 'page': 0}),\n",
      " Document(page_content='TABLE I\\nCAPABILITIES OF CHAT ASSISTANTS\\nCapability Description Underlying technique\\nChatbot persona Talk with a specified persona over chat with conversational\\nsafeguardsPersona prompting, few-shot examples, model finetuning,\\nconversation starter templates\\nKnowledge-based chat Talk to the chatbot that can access and understand multimodal\\ndocumentsRetrieval Augmented Generation (RAG) and model finetuning\\nChat memory Chatbot can remember earlier discussions Context serialized into prompt (part of grounding) and RAG\\nSemantic planning Chatbot able to generate train-of-thought and action plans\\ntowards a specific targetPrompt (and semantic skills) engineering, intent analysis\\nExternal integrations Chatbot is able to execute API calls to targets Copilot orchestration middleware and openAPI or DSL-based\\nplugins, fine-tuning with application DSLs\\nWeb search Chatbot can look up websites using search engines Search query semantic skills and web crawler middleware\\nIntegration of Copilot Chatbot can be integrated into websites, messaging cross-\\nplatforms and desktop applicationsAPI web services and cross-platform frontends for the copilot\\nof an Arrowhead Local Cloud , where the colored objects\\nare the Core Systems of the framework (Service Registry,\\nOrchestrator and Authorization) that govern the rest of the\\nnetworked Cyber-Physical Systems, abstracted as grey-boxed\\nApplication Systems , with a device abstraction underneath as\\nwell. The Arrowhead Management Tool supports engineers to\\nconfigure and provide dashboards for the contents of the Core\\nSystems, which uses dedicated, internal REST APIs of the\\nCore Systems. The Arrowhead Local Cloud is an on-prem ar-\\nchitected collection of microservices that do not have services\\nexposed over the Internet [10], and which communicate via\\na detached Public Key Infrastructure, using mutual Transport\\nLayer Security (mTLS) authentication [11].\\nFig. 1. A generic overview on an Arrowhead local cloud [10]\\nChapter II discusses the technological state of the art (SotA)\\nof Table I. Chapter III details the implemented an Arrowhead-\\nbased proof-of-concept (PoC) demonstrator application and\\npresents its findings and future work, while Chapter IV con-\\ncludes the paper.\\nII. R ELATED WORKS\\nA. LLM Engineering Used in Chat Assistants\\nWhile classic chatbot engineering already had custom top-\\nics, workflows and skills, their user journeys were mostly\\nscripted, leaving minimal reactive dynamism in their use.\\nThis all changes now with LLMs, as they are capable of\\ncontent and reasoning generation on a much higher level. Withproper prompt engineering and supporting LLM orchestration\\nmiddleware, we can build applications that make semantic\\ndecisions based on a knowledge base and various real-time\\nintegrations as well.\\nIn general, LLMs take user prompts (of a maximum size)\\nand generate responses to them. With ChatGPT 4 [1] and\\nGoogle Gemini [12], the input prompt can even be multi-\\nmodal, i.e. feature image or video files with obvious restric-\\ntions (e.g. on length or size). The chat message prompts are\\ntypically structured into an array of messages annotated with\\neither a (i) system, (ii) user, (iii) assistant or (iv) other role.\\nDevelopers can specify system prompts that are always pre-\\nappended to user prompts, and enable the developer to specify\\nthe assistant’s persona and provide other general instructions\\nfor the assistant responses. This system part needs to be\\ncarefully (prompt-)engineered to restrict the behavior of the\\nLLM. System prompts can also be used to specify many\\nthings, e.g. enforce what syntax highlights to be used in\\nthe responses or to differentiate between e.g. generated code\\nsnippets, markup documents or citations from plain text.\\nUser prompts must be pre-processed before being submitted\\nto the LLM. Besides checking it against various policies and\\nsafeguards (e.g. foul language), security scans (e.g. injections\\norjailbreaks ) [13], chat copilots need to understand the actual', metadata={'source': './copilotRC.pdf', 'page': 1}),\n",
      " Document(page_content='to the LLM. Besides checking it against various policies and\\nsafeguards (e.g. foul language), security scans (e.g. injections\\norjailbreaks ) [13], chat copilots need to understand the actual\\nintent behind the prompt (i.e. intent analysis) and then provide\\nthe necessary groundings for the LLM to accomplish the user\\nrequest. Groundings refer to all contextual details added by the\\ncopilot engine to the prompt, such as user details, relevant chat\\nhistory, or even user rights (i.e. potential callable services).\\nThis can help avoiding hallucinations and provide the LLM\\nwith guardrails in its reasoning. When attempting to establish\\nwhether or not something in an LLM response is ’true’, we\\ncan either check for it in the supplied prompt (this is called\\n’narrow grounding’) or use some general knowledge (’broad\\ngrounding’) [8].\\nAnother technique we use is called Retrieval Augmented\\nGeneration (RAG) [14]. This is also a form of grounding, but\\nbased on a knowledge base we built. This may be generated\\nfrom multimodal documents where the content is processed\\nby an embedding model and stored as a vector in a vector', metadata={'source': './copilotRC.pdf', 'page': 1}),\n",
      " Document(page_content='database [15], as depicted in Fig. 2. During user interactions,\\nthe co-pilot middleware queries the vector database to find the\\nmost relevant snippet(s) of the knowledge base, based on the\\nsemantic embedding and the understanding of the intent from\\nthe user prompt, and adds these snippets the final prompt.\\nAfterwards, based on these injected groundings, the LLM can\\nmake a more educated answer. RAG is an active research area,\\nwhere the ability to properly reason depends on numerous\\nfactors [16], [17]. There is no single implementation of a\\nRAG solution that can work well across the board, since\\nthe design of the data pipeline, the source materials, the\\nembedding process and the retrieval process itself all affect\\nits dependability.\\nIn case very large ready-made materials, knowledge base\\nand Q&A are available for use in a specific task of the co-\\npilot, we could also finetune our LLM for the purpose, but\\nthis technique is not considered in this work.\\nFig. 2. Overview of Retrieval Augmented Generation\\nLLMs are great at working with Domain Specific Languages\\n(DSL). Out-of-the-box, or based on few-shot examples, LLMs\\ncan correctly understand and then generate syntactically and\\nsemantically correct DSL artifacts. These include openAPI\\nspecifications [18] as well. This allows for external integra-\\ntions, i.e. allows the LLM to interact with public or private\\nRESTful APIs and hence act on behalf of the end user (using\\ne.g. oAuth2 authentication). This is enabled by the co-pilot\\nmiddleware solution, where the middleware is able to (i) serve\\nthe LLM the potential operations from the openAPI manifest,\\n(ii) inject the details of an operation selected,(iii) execute\\nthe API call specified by the LLM, (iv) serve the LLM the\\nresponse from the API call(s).\\nThese functions are called plugins and, together with other\\nsemantic skills , form the core of co-pilot applications. Seman-\\ntic skills are prompt templates that can be filled up by the\\nco-pilot middleware to get any AI model to execute specific\\ntasks (e.g. solve math problems correctly or tell jokes). Plugins\\nand skills have a prompt template that declares their input and\\noutput, e.g.:\\n[AVAILABLE FUNCTIONS]\\nName: ’MathPlugin-Ad’\\nDescription: ’Adds two numbers’\\nInputs:\\n-number1:double-’The first number to add’\\n-number2:double-’The second number to add’\\nOutputs:\\n-number3:double-’Sum of the input’\\nIn chat assistants, the main goal is to create chatbots that canunderstand complex tasks, plan and execute appropriate work-\\nflows on their own, and understand the responses from external\\nservices, while all based on the natural language chat with\\nits users. This is essentially the application of plan-and-solve\\nprompting, but allowing the assistant to acquire additional,\\nexternal information if needed, or act on behalf of the user. The\\nassistant can essentially propose an execution plan (course-to-\\nfine) and then act upon it. There are already existing prompting\\npatterns available for planners in LLMs, such as Sequential\\nPlanners [8], ReAct [19], Plan-and-Solve [20], RestGPT[21]\\nor Reasoning-via-Planning [22].\\nFinally, the User Experience (UX) of these assistants is just\\nas essential, since the user shall be able to view and potentially\\nmodify the plans proposed by the assistant and follow the\\nplugin calls made on behalf of the user as well. In case of API\\ncalls, the user should have the ability to review and modify\\nthe requests to-be-sent in the plan. It is also essential to allow\\nfor telemetry collection, i.e. users scoring the responses and\\nactions of the assistant. The telemetry can later on be reused\\nfor finetuning as part of the LLM-ML Ops pipelines.\\nB. Copilot Products on the Market\\nThere are numerous commercial products on the market\\nalready, capable of some of the features mentioned in the\\nprevious sections. Most notable ones are OpenAI’s ChatGPT\\nAssistant [1], Google Bard [12] and Microsoft Copilot Stu-\\ndio [23]. These are public Software-as-a-Service offerings,', metadata={'source': './copilotRC.pdf', 'page': 2}),\n",
      " Document(page_content='previous sections. Most notable ones are OpenAI’s ChatGPT\\nAssistant [1], Google Bard [12] and Microsoft Copilot Stu-\\ndio [23]. These are public Software-as-a-Service offerings,\\nand offer no-code platforms for developing chat assistants.\\nHowever, these cutting-edge services have some shortcomings\\nfor our specific industrial use case.\\n•Underlying solutions are rapidly evolving with breaking\\nchanges, as the platforms mature.\\n•Can only integrate to public API services over the Inter-\\nnet, signed by a global Certificate Authority. They do not\\nsupport mTLS.\\n•They don’t allow customization of the underlying services\\nand capabilities, such as RAG pipeline or planner.\\n•They currently do not allow users to customize UX.\\n•Such SaaS solutions generally cannot be deployed on-\\nprem.\\nIII. C OPILOTS ACROSS THE SOS L IFECYCLE\\nA. Use Cases for Arrowhead Copilots\\nBased on this, we can establish our use cases for co-pilots\\nin the Arrowhead Engineering Process (AEP) [24] for CPSoS.\\nThe mission is the ability to design and manage a CPSoS via\\nnatural language and analyze its status and monitoring data\\nusing the potentially multi-modal output of the Copilot(s) . We\\ncan conceptualize three stages of Copilots in AEP (see Fig. 3),\\nwith increasing complexity and value-added:\\n1) An Arrowhead Expert providing a chat-based entry\\npoint into the Arrowhead architecture, feeding on the\\nalready existing documentation and publications around', metadata={'source': './copilotRC.pdf', 'page': 2}),\n",
      " Document(page_content='Fig. 3. The graphical overview of the Arrowhead Engineering Process (AEP) [24] to be supported by Co-pilots\\nthe ecosystem, answering design and integration-related\\nquestions. This can be embedded in the Arrowhead\\nFramework Wiki [9] as an inline chatbot. Intended users\\nare anyone who visits the Wiki.\\n2) The Arrowhead Management Copilot interacting with\\nvarious Arrowhead Core Systems of a Local Cloud de-\\nployment to analyze and understand, potentially manage\\nthe CPSoS via the Arrowhead governing middleware.\\nThis tool can be embedded as a widget to the Arrowhead\\nManagement Tool GUI. Intended users are the authen-\\nticated Local Cloud (SoS) operators.\\n3)Arrowhead Design Copilot which can integrate with the\\nengineering toolchain to design SoS deployment and\\nunderlying industrial automation processes and infras-\\ntructure (i.e. SysML modeling with Eclipse Papyrus).\\nThis co-pilot can be integrated into the Arrowhead\\nEngineering Toolchain, interacting with the design tools\\nfor CPSoS. The intended users are the SoS engineers.\\nThis work demonstrates the first and second use cases,\\nand these require fairly different setups and complexity. The\\nArrowhead Expert is primarily a RAG-oriented Q&A assistant,\\nwithout the need to use long-term memory. Fig. 4 depicts the\\nRAG solution components, as demonstrator uses Microsoft\\nAzure resources for persistence, and relies on the Semantic\\nKernel middleware. Its primary knowledge base comes from\\nthe Arrowhead documentation and research papers (in IEEE\\nformat). We have also provided a few-shot training on sam-\\nple question-answers, but we primarily rely on the persona\\nprompting pattern to define how the Expert shall conduct itself\\nin general:\\nThis is a chat between an intelligent chatbot named Ar-\\nrowhead Expert and one human participant. The Arrowhead\\nExpert is an expert in the Arrowhead framework and can\\naccess Arrowhead documentation and publications. The user\\nwould like to use Arrowhead for engineering (CPSoS), so help\\nthem learn and adapt the Arrowhead framework. Try to be\\nconcise with your answers, and rely only on the documentation\\nsnippets provided below. If you don’t know the answer, don’t\\nmake things up, but say so. Do not answer questions outside\\nof the Arrowhead context.\\nThis expert is able to answer reasonable questions like\\n”What are the restrictions on system naming in Arrowhead?” ,\\nor”How can manually generate certificates for a new appli-\\ncation system, if I do not have Onboarding Controller?” . It\\nwill not remember users after the session, or their chat history\\n(only up until chaining previous messages of the session can\\nFig. 4. Overview of the Arrowhead Expert\\nfit into the prompt as context), but it is also not necessary.\\nHowever, it can provide the relevant references to the docu-\\nmentation, where the answer can be further researched. Its\\ninitial knowledge base consists of Arrowhead publications,\\nAPI and developer documentation of the core systems, and\\nthe contents of the Arrowhead Wiki, altogether 42 documents\\nfor the PoC.\\nFig. 5. Overview of the Arrowhead Management Copilot\\nMeanwhile, the Arrowhead Management Co-pilot is a more\\ncomplex assistant, as depicted in Fig. 5. It needs to (i) be able\\nto plan, (ii) remember chat context for better planning, and\\n(iii) execute API calls against the Arrowhead Core Systems.\\nThis application is deployed with Azure private resources,\\nsince proper AAA is missing yet in the implementation. The\\nvector database, embedding model and blob storage are needed\\nfor storing long-term chat and document-based memory (and\\nenable RAG on top of them), while CosmosDB stores the\\nchat interactions and the plans proposed by the bot with other', metadata={'source': './copilotRC.pdf', 'page': 3}),\n",
      " Document(page_content='configuration items (like persona prompts, etc.). The solution\\nis deployed in the same private network as the Arrowhead core\\nsystems. The persona of this co-pilot is phrased as follows:\\nThis is a chat between an intelligent assistant named\\nArrowhead Co-pilot and a human participant. The Arrowhead\\nCo-pilot is an expert in Arrowhead framework and can access\\nthe Arrowhead core systems via API plugins. Try to be concise\\nwith your answers, and primarily rely on the Arrowhead\\ncore systems for providing up-to-date data and configuration\\nfrom the system of systems. If you don’t know the answer or\\ndon’t know how you can access the requested information,\\ndon’t make things up, but say so. You can also ask follow-up\\nquestions for clarification on what is required by the user.\\nThis assistant is able to answer simple questions like ”What\\nare the currently registered services in the Arrowhead Ser-\\nvice Registry?” , but it can also chain API calls together in\\nsequential plans, if we require more complicated tasks, such\\nas”Please add an authorization rule between systems P and\\nC, for service S in the Arrowhead intra-cloud Authorization\\nSystem, but please register system X as the provider of service\\nS in the Arrowhead Service Registry first” .\\nB. Findings of the Proof of Concept\\nThese PoC applications showcase good initial impression.\\nValidation of the PoC has been done in a small workshop with\\npotential stakeholders. Both assistants can handle Arrowhead-\\ncontext and act as their appropriate personas with the intended\\nguardrails. The document RAG, long-term memory, plugin\\nhandling (i.e. making API calls to Arrowhead) and basic\\nplanner features are well received. The general UX, however,\\nis inadequate to our industrial purposes and response times are\\nconsidered long.\\nThe document RAG, with the current semantic memory\\ncapabilities, does not allow for utilizing the multi-modality\\nof (mostly IEEE-formatted) scientific publications and Arrow-\\nhead documentation. This RAG engine, while works accept-\\nably acting as long-term chat memory, is unable to work with\\ntables or diagrams of papers, which is a great loss of semantic\\ncontent. This renders the Expert unable to help with e.g. the\\nArrowhead SoS modeling concepts and visualization tasks.\\nMoreover, the somewhat ”naive” document chunking strategy\\n(i.e., every 1000 characters) is seriously limiting usability, as\\nwords are often even cut in half. Another related problem,\\nalbeit humorous, is that publications often contain (accidental)\\nhyphenations, which causes problems in understanding techni-\\ncal specifications (i.e. in certificate CN names: arrowhead.eu\\nvs.arrow-head.eu ).\\nConnecting the LLMs with the Arrowhead APIs also brings\\npractical challenges. Plugins utilize openAPI3.0 specifications,\\ni.e. they work with the operationId and description of each\\nendpoint first, then with the path templates, headers, request\\nand response structures. It apparently does not bode well with\\nthe LLM, if the openAPI specification is ”too” long (even if\\nit fits the token limit) or contains a lot of seemingly similarendpoints. It can also cause unfortunate ”misunderstandings”\\nif path templates or JSON objects have depth (e.g. over 3) to\\nthem, or if the openAPI document describes them in an object-\\noriented manner (e.g. using AllOf, AnyOf directives). An\\nadditional problem is when whole JSON objects or values can\\nstill be hallucinated, even if the prompt seems well grounded.\\nOpenAI has published a guideline on plugin openAPI speci-\\nfications, but this could warrant further research [25].\\nMoreover, the assistant is able to put together API calls\\nfrom different plugins as well (i.e. register services with the\\nService Registry first, then use the acquired IDs to register\\nrelated authorization rules in Authorization System) in the\\nplans. However, it is not able to handle making the logical\\nconnections between the same IDs of Arrowhead objects if\\nthose are served via different plugins. This assumes that the', metadata={'source': './copilotRC.pdf', 'page': 4}),\n",
      " Document(page_content='plans. However, it is not able to handle making the logical\\nconnections between the same IDs of Arrowhead objects if\\nthose are served via different plugins. This assumes that the\\nLLM is only able to contextualize plugins individually, and\\nhence requires plugin API designs that are self-sufficient. It is\\nalso clear that plans are static and the co-pilot has no chance\\nof making adjustments after subsequent API calls are made.\\nRegarding XAI, the current UI design allows for a minimal\\nview to the behind-the-scenes. Nevertheless, the visualization\\nof the technical prompts, plans and document citations in\\nresponses need extra consideration. Moreover, the assistant\\nshould be able to respond to follow-up questions on previous\\nplans and results, but the short-term memory cannot produce\\nwell-structured output (missing grounding for understanding\\npreviously generated internal data structures in the follow-up\\nprompt).\\nC. Future Work\\nThere is significant design, implementation and research\\nwork lying ahead to mature these co-pilots. The Arrow-\\nhead Expert shall be deployed with cloud-native resources,\\nbut needs to implement rate limiting and security harden-\\ning (e.g. against prompt injections) on its interactions to\\navoid anonymous jailbreaks. It also shall feature industrial-\\ngrade AAA (authentication-authorisation-accounting), while\\nintegrating into the Arrowhead PKI.\\nMeanwhile, the final implementation needs to move away\\nfrom Azure dependencies, i.e. it needs to support on-prem\\ndeployment models and enable the choice of upstream ser-\\nvices. It also needs to allow for small on-prem and cloud-\\nnative deployments, and will be implemented as an API First\\nheadless architecture.\\nOur RAG pipeline needs a complete design, where it needs\\nto be multi-modal and to process markup documents better\\n(Arrowhead documentation) and research publications (i.e.\\nIEEE formatted). The focus must be put on teaching the\\nExpert about the Arrowhead’s UML-based modeling paradigm\\nas well. Regarding the test automation framework for RAG,\\nwe need to establish a validation Q&A dataset with Arrowhead\\ncontext, perhaps from real-life interactions between users and\\nthe PoC. Editors of the knowledge base also need automated\\nmechanisms to be able to review the chunking process and', metadata={'source': './copilotRC.pdf', 'page': 4}),\n",
      " Document(page_content='update documents, if needed. Afterwards, the plan is to build a\\nprompt flow-based test automation framework, where answers\\nto test scenarios are to be evaluated by another customized\\nGPT, and act as a quality gate in our knowledge base-related\\nCICD (Cont. Integration and Delivery pipelines).\\nThe Expert and Management Co-pilot needs to become\\nmulti-modal as well, i.e. able to handle and use visualizations\\nin their interactions. Therefore, generating tables out of plan\\nresults, visualizing graphs and UML diagrams from the Ar-\\nrowhead runtime are on the backlog. Other XAI-related UX\\nchanges will be implemented as well, where users shall be able\\nto track and modify plans, and review and edit raw prompts\\nand API calls made by the plugins to upstream services. User\\nfeedback and telemetry collection will allow for collecting\\npotential future test scenarios. Reinforcement learning is only\\na long-term opportunity.\\nThe planner engine requires work, and should be based\\non more advanced prompt engineering techniques, like Re-\\nAct [19] or interactive planner [26]. After each step, the user\\nand the LLM should have the ability to update the next API\\ncalls to be made, based on the results from previous calls.\\nThe planner engine could also utilize few-shot learning, as\\nindicated in LLM planner surveys [27].\\nFinally, the Arrowhead framework itself needs additional\\nwork as well to integrate better with co-pilots. Each core\\nservice requires additional endpoints and business logic to\\nserve compliant to the openAI plugin specifications [25].\\nMoreover, the individual API designs need to adapt and feature\\nbetter segregation of duties, so LLMs can understand available\\nmethods and their templates better.\\nIV. C ONCLUSIONS\\nThe design of LLM-based applications, like chat assistants,\\nis a topic of day-by-day rapid evolution. From idea formation\\nto productizing the solution, there are no existing industrial\\nbest practices or really reusable frameworks yet. This paper\\ndescribes the first steps in the process of creating chat co-\\npilots for the CPSoS engineering domain, starting by analyzing\\nachievable capabilities offered by LLM SotA, establishing\\nfeasible use cases for the technology, creating a proof-of-\\nconcept implementation; up to specifying an R&D roadmap,\\nall in the context of the Arrowhead framework.\\nACKNOWLEDGMENT\\nProject No. KDP-IKT-2023-900-I1-00000957/0000003 has\\nbeen implemented with the support provided by the Ministry\\nof Culture and Innovation of Hungary from the National\\nResearch, Development and Innovation Fund, financed by the\\nCo-operative Doctoral Program [C2299296] funding scheme.\\nPart of this work is created within the Arrowhead fPVN\\nproject, supported by the Chips-JU under grant agreement no.\\n101111977.REFERENCES\\n[1] OpenAI, “Chatgpt: Optimizing language models for dialogue,” https:\\n//www.openai.com/chatgpt, 2023, acc: 2023-12-28.\\n[2] C. Parnin, G. Soares, R. Pandita, S. Gulwani, J. Rich, and A. Z. Henley,\\n“Building your own product copilot: Challenges, opportunities, and\\nneeds,” 2023.\\n[3] Microsoft, “365 Copilot,” https://learn.microsoft.com/en-us/\\nmicrosoft-365-copilot/microsoft-365-copilot-overview, 2023, acc:\\n2023-12-28.\\n[4] P. J. Phillips, C. A. Hahn, P. C. Fontana, D. A. Broniatowski, and\\nM. A. Przybocki, “Four principles of explainable artificial intelligence,”\\nNISTIR, Gaithersburg, Maryland , vol. 18, 2020.\\n[5] LangChain, “LLM Orchestration,” https://www.langchain.com/, 2023,\\nacc: 2023-12-28.\\n[6] Llamaindex, “Data framework for llm applications,” https://www.\\nllamaindex.ai/, 2023, acc: 2023-12-28.\\n[7] DeepSet AI, “Haystack,” https://github.com/deepset-ai/haystack, 2023,\\nacc: 2023-12-28.\\n[8] Microsoft, “Semantic kernel,” https://github.com/microsoft/\\nsemantic-kernel, 2023, acc: 2023-12-28.\\n[9] Eclipse Foundation, “The Arrowhead Framework,” https://projects.\\neclipse.org/projects/iot.arrowhead.\\n[10] C. Hegedus, P. Varga, and S. Tanyi, “A governance model for local and\\ninterconnecting arrowhead clouds,” 10 2020.', metadata={'source': './copilotRC.pdf', 'page': 5}),\n",
      " Document(page_content='eclipse.org/projects/iot.arrowhead.\\n[10] C. Hegedus, P. Varga, and S. Tanyi, “A governance model for local and\\ninterconnecting arrowhead clouds,” 10 2020.\\n[11] S. Pl ´osz, C. Hegedus, and P. Varga, “Advanced security considerations\\nin the arrowhead framework,” vol. 9923, 09 2016, pp. 234–245.\\n[12] Google, “Bard,” https://bard.google.com/, 08 2023.\\n[13] OWASP, “Top 10 for LLM Applications,” https://llmtop10.com/, acc:\\n2023-12-28.\\n[14] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana,\\nand S. Nanayakkara, “Improving the Domain Adaptation of Retrieval\\nAugmented Generation (RAG) Models for Open Domain Question An-\\nswering,” Transactions of the Association for Computational Linguistics ,\\nvol. 11, pp. 1–17, 01 2023.\\n[15] I. Ilin, “Advanced RAG Techniques: an Il-\\nlustrated Overview,” https://pub.towardsai.net/\\nadvanced-rag-techniques-an-illustrated-overview-04d193d8fec6.\\n[16] P. BehnamGhader, S. Miret, and S. Reddy, “Can retriever-augmented\\nlanguage models reason? the blame game between the retriever and the\\nlanguage model,” 2023.\\n[17] Z. Shao, Y . Gong, Y . Shen, M. Huang, N. Duan, and W. Chen,\\n“Enhancing retrieval-augmented large language models with iterative\\nretrieval-generation synergy,” 2023.\\n[18] OpenAPI Initiative, “The openapi specification,” https://www.openapis.\\norg/, acc: 2023-12-28.\\n[19] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao,\\n“React: Synergizing reasoning and acting in language models,” 2023.\\n[20] L. Wang, W. Xu, Y . Lan, Z. Hu, Y . Lan, R. K.-W. Lee, and E.-P.\\nLim, “Plan-and-solve prompting: Improving zero-shot chain-of-thought\\nreasoning by large language models,” 2023.\\n[21] Y . Song, W. Xiong, D. Zhu, W. Wu, H. Qian, M. Song, H. Huang,\\nC. Li, K. Wang, R. Yao, Y . Tian, and S. Li, “Restgpt: Connecting large\\nlanguage models with real-world restful apis,” 2023.\\n[22] S. Hao, Y . Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu,\\n“Reasoning with language model is planning with world model,” 2023.\\n[23] Microsoft, “Copilot Studio,” https://www.microsoft.com/en-us/\\nmicrosoft-copilot/microsoft-copilot-studio, 2023, acc: 2023-12-28.\\n[24] G. Kulcs ´ar, P. Varga, M. S. Tatara, F. Montori, M. A. Inigo, G. Urgese,\\nand P. Azzoni, “Modeling an industrial revolution: How to manage\\nlarge-scale, complex iot ecosystems?” in 2021 IFIP/IEEE International\\nSymposium on Integrated Network Management (IM) . IEEE, 2021.\\n[25] OpenAI, “ChatGPT Plugins : Getting Started,” https://platform.openai.\\ncom/docs/plugins/getting-started, 2023, acc: 2023-12-28.\\n[26] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and\\nY . Su, “Llm-planner: Few-shot grounded planning for embodied agents\\nwith large language models,” 2023.\\n[27] S. Qiao, Y . Ou, N. Zhang, X. Chen, Y . Yao, S. Deng, C. Tan, F. Huang,\\nand H. Chen, “Reasoning with language model prompting: A survey,”\\n2023.', metadata={'source': './copilotRC.pdf', 'page': 5})]\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from pprint import pprint\n",
    "file_path = \"./copilotRC.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load_and_split()\n",
    "pprint(pages)\n",
    "print(len(pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to split the large Documents into smaller chunks that can be later injected into prompts. \n",
    "\n",
    "It's worth noting that currently, gpt4o supports roughly 120K tokens as input context window. This will be filled with:\n",
    "\n",
    "* system prompt\n",
    "* chat history\n",
    "* user query\n",
    "* context injected by RAG pipeline\n",
    "\n",
    "We usually inject a couple of relevant chunks, let's say 3. Therefore, we should have chunks that are around 10k tokens each. Previously (GPT4-32K or GPT-3.5), this chunk size was much-much smaller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Co-pilots for Arrowhead-based\\nCyber-Physical System of Systems Engineering\\nCsaba Heged ˝us, P ´al Varga\\nDepartment of Telecommunications and Artificial Intelligence\\nBudapest University of Technology and Economics\\nM˝uegyetem rkp. 3., H-1111 Budapest, Hungary.\\nEmail: {hegeduscs, pvarga }@tmit.bme.hu\\nAbstract —One benefit of Large Language Model (LLM) based\\napplications (e.g. chat assistants or co-pilots) is that they can\\nbring humans closer to the loop in various IT and OT solutions.\\nCo-pilots can achieve many things at once, i.e. provide a context-\\naware natural language interface to knowledge bases, reach\\nvarious systems (via APIs), or even help solving multi-step\\nproblems with their planning and reasoning abilities. However,\\nmaking production-grade chat assistants is a topical challenge,\\nas fast-evolving LLMs expose new types of application design\\nand security issues that need tackling. These especially rise\\nto power when we try to apply these solutions to industrial', metadata={'source': './copilotRC.pdf', 'page': 0}),\n",
      " Document(page_content='as fast-evolving LLMs expose new types of application design\\nand security issues that need tackling. These especially rise\\nto power when we try to apply these solutions to industrial\\nautomation use cases, as they need additional explainability and\\nreliability engineered into the architecture. This paper describes\\nthe envisioned use cases and the findings of proof of concept\\ncopilots for the Cyber-Physical System of Systems (CPSoS)\\nengineering domain. The paper suggests three types of copilots to\\nsupport the stages throughout the CPSoS engineering lifecycle –\\nand shows Proof-of-Concept scenarios for the Eclipse Arrowhead\\nengineering process.\\nI. I NTRODUCTION\\nThe year 2023 has mostly been spent around generative\\nArtificial Intelligence (genAI) and Large Language Models\\n(LLM). Various commercial products and open-source projects\\nhave appeared and are showing great evolution in just a\\nvery short timeframe. Most notable ones include the products', metadata={'source': './copilotRC.pdf', 'page': 0}),\n",
      " Document(page_content='(LLM). Various commercial products and open-source projects\\nhave appeared and are showing great evolution in just a\\nvery short timeframe. Most notable ones include the products\\nof OpenAI and Microsoft, ChatGPT 4 and its text-to-image\\nmodel Dall-E [1].\\nNow, every industry and domain is looking at how to\\nutilize these technologies in their business and keep up with\\ntheir development speed in the productization phases. Building\\nsolutions with these products capable of semantic reasoning\\nand contextual content generation, among others, enables a lot\\nof new ideas to come forth from product owners [2]. Many\\ncompanies, Microsoft for example, are building their LLM-\\nand genAI-based product portfolio around the concept of co-\\npilots .\\nCo-pilots, or chat assistants, refer to an AI-based solution\\naiding users in various customer journeys of an ecosystem.\\nFor example, the Microsoft Copilot [3] suite is a horizontal\\nsolution in the Microsoft 365 and Azure ecosystems that has', metadata={'source': './copilotRC.pdf', 'page': 0}),\n",
      " Document(page_content='aiding users in various customer journeys of an ecosystem.\\nFor example, the Microsoft Copilot [3] suite is a horizontal\\nsolution in the Microsoft 365 and Azure ecosystems that has\\naccess to the user’s (and organization’s) data in Azure and\\nEntra ID , and can interact with various Microsoft 365 (Office,\\nPower and Azure platform) applications via Application Pro-\\ngramming Interfaces (APIs) and specialized Domain SpecificLanguages (DSLs) to generate, summarise or augment (multi-\\nmodal) content; or act on behalf of the user within these\\napplications.\\nNevertheless, rightful concerns have arisen around the\\nsafety, reliability and the responsible use of all AI-based\\nsolutions. Explainable AI (XAI) emphasizes transparent model\\narchitectures and feature importance rankings, as it is essential\\nto comprehend the reasoning behind the AI’s decisions and\\nactions. This is especially the case for potential industrial\\n(manufacturing) applications, as ours. Our chat assistant needs', metadata={'source': './copilotRC.pdf', 'page': 0}),\n",
      " Document(page_content='to comprehend the reasoning behind the AI’s decisions and\\nactions. This is especially the case for potential industrial\\n(manufacturing) applications, as ours. Our chat assistant needs\\nto be able to provide plans and explanations that consider the\\nspecific industrial domain knowledge, so its users can trust\\nand validate the outcome [4].\\nA. Capabilities of Chat Assistants\\nBased on the current state of the business, these chat-based\\nco-pilot agents can have many interesting features. Primarily,\\nthe capabilities depicted in Table I are considered for this\\npaper. Besides these, there is one additional capability worth\\nmentioning: the code interpreter sandbox, where the model can\\nrun generated code snippets against uploaded files. However,\\nthis feature is out of the scope of this paper.', metadata={'source': './copilotRC.pdf', 'page': 0}),\n",
      " Document(page_content='mentioning: the code interpreter sandbox, where the model can\\nrun generated code snippets against uploaded files. However,\\nthis feature is out of the scope of this paper.\\nMost of these functionalities can be developed using mod-\\nern, cloud-native application design patterns and LLM applica-\\ntion orchestration platforms, such as LangChain [5], llamain-\\ndex [6], Haystack [7] or Semantic Kernel and Memory (from\\nMicrosoft) [8].\\nB. Motivation and Structure of the Paper\\nThe purpose of this paper is to demonstrate that LLM-based\\nco-pilots can be implemented for industrial automation as\\nwell, to aid the development and operations of large-scale Sys-\\ntem of Systems (SoS). We will also report on the qualitative\\nfindings around a proof-of-concept (PoC) demonstrator that\\nintegrates into the framework and showcases the values added\\nacross the Arrowhead toolchain.\\nThis work is to be considered in the context of the Eclipse\\nArrowhead framework [9], which is a Service Oriented Archi-', metadata={'source': './copilotRC.pdf', 'page': 0}),\n",
      " Document(page_content='across the Arrowhead toolchain.\\nThis work is to be considered in the context of the Eclipse\\nArrowhead framework [9], which is a Service Oriented Archi-\\ntecture (SOA) driven design framework and middleware im-\\nplementation to govern and orchestrate Cyber-Physical System\\nof systems (CPSoS). Figure 1 depicts the deployment structure', metadata={'source': './copilotRC.pdf', 'page': 0}),\n",
      " Document(page_content='TABLE I\\nCAPABILITIES OF CHAT ASSISTANTS\\nCapability Description Underlying technique\\nChatbot persona Talk with a specified persona over chat with conversational\\nsafeguardsPersona prompting, few-shot examples, model finetuning,\\nconversation starter templates\\nKnowledge-based chat Talk to the chatbot that can access and understand multimodal\\ndocumentsRetrieval Augmented Generation (RAG) and model finetuning\\nChat memory Chatbot can remember earlier discussions Context serialized into prompt (part of grounding) and RAG\\nSemantic planning Chatbot able to generate train-of-thought and action plans\\ntowards a specific targetPrompt (and semantic skills) engineering, intent analysis\\nExternal integrations Chatbot is able to execute API calls to targets Copilot orchestration middleware and openAPI or DSL-based\\nplugins, fine-tuning with application DSLs\\nWeb search Chatbot can look up websites using search engines Search query semantic skills and web crawler middleware', metadata={'source': './copilotRC.pdf', 'page': 1}),\n",
      " Document(page_content='plugins, fine-tuning with application DSLs\\nWeb search Chatbot can look up websites using search engines Search query semantic skills and web crawler middleware\\nIntegration of Copilot Chatbot can be integrated into websites, messaging cross-\\nplatforms and desktop applicationsAPI web services and cross-platform frontends for the copilot\\nof an Arrowhead Local Cloud , where the colored objects\\nare the Core Systems of the framework (Service Registry,\\nOrchestrator and Authorization) that govern the rest of the\\nnetworked Cyber-Physical Systems, abstracted as grey-boxed\\nApplication Systems , with a device abstraction underneath as\\nwell. The Arrowhead Management Tool supports engineers to\\nconfigure and provide dashboards for the contents of the Core\\nSystems, which uses dedicated, internal REST APIs of the\\nCore Systems. The Arrowhead Local Cloud is an on-prem ar-\\nchitected collection of microservices that do not have services\\nexposed over the Internet [10], and which communicate via', metadata={'source': './copilotRC.pdf', 'page': 1}),\n",
      " Document(page_content='Core Systems. The Arrowhead Local Cloud is an on-prem ar-\\nchitected collection of microservices that do not have services\\nexposed over the Internet [10], and which communicate via\\na detached Public Key Infrastructure, using mutual Transport\\nLayer Security (mTLS) authentication [11].\\nFig. 1. A generic overview on an Arrowhead local cloud [10]\\nChapter II discusses the technological state of the art (SotA)\\nof Table I. Chapter III details the implemented an Arrowhead-\\nbased proof-of-concept (PoC) demonstrator application and\\npresents its findings and future work, while Chapter IV con-\\ncludes the paper.\\nII. R ELATED WORKS\\nA. LLM Engineering Used in Chat Assistants\\nWhile classic chatbot engineering already had custom top-\\nics, workflows and skills, their user journeys were mostly\\nscripted, leaving minimal reactive dynamism in their use.\\nThis all changes now with LLMs, as they are capable of', metadata={'source': './copilotRC.pdf', 'page': 1}),\n",
      " Document(page_content='ics, workflows and skills, their user journeys were mostly\\nscripted, leaving minimal reactive dynamism in their use.\\nThis all changes now with LLMs, as they are capable of\\ncontent and reasoning generation on a much higher level. Withproper prompt engineering and supporting LLM orchestration\\nmiddleware, we can build applications that make semantic\\ndecisions based on a knowledge base and various real-time\\nintegrations as well.\\nIn general, LLMs take user prompts (of a maximum size)\\nand generate responses to them. With ChatGPT 4 [1] and\\nGoogle Gemini [12], the input prompt can even be multi-\\nmodal, i.e. feature image or video files with obvious restric-\\ntions (e.g. on length or size). The chat message prompts are\\ntypically structured into an array of messages annotated with\\neither a (i) system, (ii) user, (iii) assistant or (iv) other role.\\nDevelopers can specify system prompts that are always pre-\\nappended to user prompts, and enable the developer to specify', metadata={'source': './copilotRC.pdf', 'page': 1}),\n",
      " Document(page_content='either a (i) system, (ii) user, (iii) assistant or (iv) other role.\\nDevelopers can specify system prompts that are always pre-\\nappended to user prompts, and enable the developer to specify\\nthe assistant’s persona and provide other general instructions\\nfor the assistant responses. This system part needs to be\\ncarefully (prompt-)engineered to restrict the behavior of the\\nLLM. System prompts can also be used to specify many\\nthings, e.g. enforce what syntax highlights to be used in\\nthe responses or to differentiate between e.g. generated code\\nsnippets, markup documents or citations from plain text.\\nUser prompts must be pre-processed before being submitted\\nto the LLM. Besides checking it against various policies and\\nsafeguards (e.g. foul language), security scans (e.g. injections\\norjailbreaks ) [13], chat copilots need to understand the actual', metadata={'source': './copilotRC.pdf', 'page': 1}),\n",
      " Document(page_content='to the LLM. Besides checking it against various policies and\\nsafeguards (e.g. foul language), security scans (e.g. injections\\norjailbreaks ) [13], chat copilots need to understand the actual\\nintent behind the prompt (i.e. intent analysis) and then provide\\nthe necessary groundings for the LLM to accomplish the user\\nrequest. Groundings refer to all contextual details added by the\\ncopilot engine to the prompt, such as user details, relevant chat\\nhistory, or even user rights (i.e. potential callable services).\\nThis can help avoiding hallucinations and provide the LLM\\nwith guardrails in its reasoning. When attempting to establish\\nwhether or not something in an LLM response is ’true’, we\\ncan either check for it in the supplied prompt (this is called\\n’narrow grounding’) or use some general knowledge (’broad\\ngrounding’) [8].\\nAnother technique we use is called Retrieval Augmented\\nGeneration (RAG) [14]. This is also a form of grounding, but', metadata={'source': './copilotRC.pdf', 'page': 1}),\n",
      " Document(page_content='’narrow grounding’) or use some general knowledge (’broad\\ngrounding’) [8].\\nAnother technique we use is called Retrieval Augmented\\nGeneration (RAG) [14]. This is also a form of grounding, but\\nbased on a knowledge base we built. This may be generated\\nfrom multimodal documents where the content is processed\\nby an embedding model and stored as a vector in a vector', metadata={'source': './copilotRC.pdf', 'page': 1}),\n",
      " Document(page_content='database [15], as depicted in Fig. 2. During user interactions,\\nthe co-pilot middleware queries the vector database to find the\\nmost relevant snippet(s) of the knowledge base, based on the\\nsemantic embedding and the understanding of the intent from\\nthe user prompt, and adds these snippets the final prompt.\\nAfterwards, based on these injected groundings, the LLM can\\nmake a more educated answer. RAG is an active research area,\\nwhere the ability to properly reason depends on numerous\\nfactors [16], [17]. There is no single implementation of a\\nRAG solution that can work well across the board, since\\nthe design of the data pipeline, the source materials, the\\nembedding process and the retrieval process itself all affect\\nits dependability.\\nIn case very large ready-made materials, knowledge base\\nand Q&A are available for use in a specific task of the co-\\npilot, we could also finetune our LLM for the purpose, but\\nthis technique is not considered in this work.', metadata={'source': './copilotRC.pdf', 'page': 2}),\n",
      " Document(page_content='and Q&A are available for use in a specific task of the co-\\npilot, we could also finetune our LLM for the purpose, but\\nthis technique is not considered in this work.\\nFig. 2. Overview of Retrieval Augmented Generation\\nLLMs are great at working with Domain Specific Languages\\n(DSL). Out-of-the-box, or based on few-shot examples, LLMs\\ncan correctly understand and then generate syntactically and\\nsemantically correct DSL artifacts. These include openAPI\\nspecifications [18] as well. This allows for external integra-\\ntions, i.e. allows the LLM to interact with public or private\\nRESTful APIs and hence act on behalf of the end user (using\\ne.g. oAuth2 authentication). This is enabled by the co-pilot\\nmiddleware solution, where the middleware is able to (i) serve\\nthe LLM the potential operations from the openAPI manifest,\\n(ii) inject the details of an operation selected,(iii) execute\\nthe API call specified by the LLM, (iv) serve the LLM the\\nresponse from the API call(s).', metadata={'source': './copilotRC.pdf', 'page': 2}),\n",
      " Document(page_content='(ii) inject the details of an operation selected,(iii) execute\\nthe API call specified by the LLM, (iv) serve the LLM the\\nresponse from the API call(s).\\nThese functions are called plugins and, together with other\\nsemantic skills , form the core of co-pilot applications. Seman-\\ntic skills are prompt templates that can be filled up by the\\nco-pilot middleware to get any AI model to execute specific\\ntasks (e.g. solve math problems correctly or tell jokes). Plugins\\nand skills have a prompt template that declares their input and\\noutput, e.g.:\\n[AVAILABLE FUNCTIONS]\\nName: ’MathPlugin-Ad’\\nDescription: ’Adds two numbers’\\nInputs:\\n-number1:double-’The first number to add’\\n-number2:double-’The second number to add’\\nOutputs:\\n-number3:double-’Sum of the input’\\nIn chat assistants, the main goal is to create chatbots that canunderstand complex tasks, plan and execute appropriate work-\\nflows on their own, and understand the responses from external', metadata={'source': './copilotRC.pdf', 'page': 2}),\n",
      " Document(page_content='In chat assistants, the main goal is to create chatbots that canunderstand complex tasks, plan and execute appropriate work-\\nflows on their own, and understand the responses from external\\nservices, while all based on the natural language chat with\\nits users. This is essentially the application of plan-and-solve\\nprompting, but allowing the assistant to acquire additional,\\nexternal information if needed, or act on behalf of the user. The\\nassistant can essentially propose an execution plan (course-to-\\nfine) and then act upon it. There are already existing prompting\\npatterns available for planners in LLMs, such as Sequential\\nPlanners [8], ReAct [19], Plan-and-Solve [20], RestGPT[21]\\nor Reasoning-via-Planning [22].\\nFinally, the User Experience (UX) of these assistants is just\\nas essential, since the user shall be able to view and potentially\\nmodify the plans proposed by the assistant and follow the\\nplugin calls made on behalf of the user as well. In case of API', metadata={'source': './copilotRC.pdf', 'page': 2}),\n",
      " Document(page_content='as essential, since the user shall be able to view and potentially\\nmodify the plans proposed by the assistant and follow the\\nplugin calls made on behalf of the user as well. In case of API\\ncalls, the user should have the ability to review and modify\\nthe requests to-be-sent in the plan. It is also essential to allow\\nfor telemetry collection, i.e. users scoring the responses and\\nactions of the assistant. The telemetry can later on be reused\\nfor finetuning as part of the LLM-ML Ops pipelines.\\nB. Copilot Products on the Market\\nThere are numerous commercial products on the market\\nalready, capable of some of the features mentioned in the\\nprevious sections. Most notable ones are OpenAI’s ChatGPT\\nAssistant [1], Google Bard [12] and Microsoft Copilot Stu-\\ndio [23]. These are public Software-as-a-Service offerings,', metadata={'source': './copilotRC.pdf', 'page': 2}),\n",
      " Document(page_content='previous sections. Most notable ones are OpenAI’s ChatGPT\\nAssistant [1], Google Bard [12] and Microsoft Copilot Stu-\\ndio [23]. These are public Software-as-a-Service offerings,\\nand offer no-code platforms for developing chat assistants.\\nHowever, these cutting-edge services have some shortcomings\\nfor our specific industrial use case.\\n•Underlying solutions are rapidly evolving with breaking\\nchanges, as the platforms mature.\\n•Can only integrate to public API services over the Inter-\\nnet, signed by a global Certificate Authority. They do not\\nsupport mTLS.\\n•They don’t allow customization of the underlying services\\nand capabilities, such as RAG pipeline or planner.\\n•They currently do not allow users to customize UX.\\n•Such SaaS solutions generally cannot be deployed on-\\nprem.\\nIII. C OPILOTS ACROSS THE SOS L IFECYCLE\\nA. Use Cases for Arrowhead Copilots\\nBased on this, we can establish our use cases for co-pilots\\nin the Arrowhead Engineering Process (AEP) [24] for CPSoS.', metadata={'source': './copilotRC.pdf', 'page': 2}),\n",
      " Document(page_content='III. C OPILOTS ACROSS THE SOS L IFECYCLE\\nA. Use Cases for Arrowhead Copilots\\nBased on this, we can establish our use cases for co-pilots\\nin the Arrowhead Engineering Process (AEP) [24] for CPSoS.\\nThe mission is the ability to design and manage a CPSoS via\\nnatural language and analyze its status and monitoring data\\nusing the potentially multi-modal output of the Copilot(s) . We\\ncan conceptualize three stages of Copilots in AEP (see Fig. 3),\\nwith increasing complexity and value-added:\\n1) An Arrowhead Expert providing a chat-based entry\\npoint into the Arrowhead architecture, feeding on the\\nalready existing documentation and publications around', metadata={'source': './copilotRC.pdf', 'page': 2}),\n",
      " Document(page_content='Fig. 3. The graphical overview of the Arrowhead Engineering Process (AEP) [24] to be supported by Co-pilots\\nthe ecosystem, answering design and integration-related\\nquestions. This can be embedded in the Arrowhead\\nFramework Wiki [9] as an inline chatbot. Intended users\\nare anyone who visits the Wiki.\\n2) The Arrowhead Management Copilot interacting with\\nvarious Arrowhead Core Systems of a Local Cloud de-\\nployment to analyze and understand, potentially manage\\nthe CPSoS via the Arrowhead governing middleware.\\nThis tool can be embedded as a widget to the Arrowhead\\nManagement Tool GUI. Intended users are the authen-\\nticated Local Cloud (SoS) operators.\\n3)Arrowhead Design Copilot which can integrate with the\\nengineering toolchain to design SoS deployment and\\nunderlying industrial automation processes and infras-\\ntructure (i.e. SysML modeling with Eclipse Papyrus).\\nThis co-pilot can be integrated into the Arrowhead\\nEngineering Toolchain, interacting with the design tools', metadata={'source': './copilotRC.pdf', 'page': 3}),\n",
      " Document(page_content='tructure (i.e. SysML modeling with Eclipse Papyrus).\\nThis co-pilot can be integrated into the Arrowhead\\nEngineering Toolchain, interacting with the design tools\\nfor CPSoS. The intended users are the SoS engineers.\\nThis work demonstrates the first and second use cases,\\nand these require fairly different setups and complexity. The\\nArrowhead Expert is primarily a RAG-oriented Q&A assistant,\\nwithout the need to use long-term memory. Fig. 4 depicts the\\nRAG solution components, as demonstrator uses Microsoft\\nAzure resources for persistence, and relies on the Semantic\\nKernel middleware. Its primary knowledge base comes from\\nthe Arrowhead documentation and research papers (in IEEE\\nformat). We have also provided a few-shot training on sam-\\nple question-answers, but we primarily rely on the persona\\nprompting pattern to define how the Expert shall conduct itself\\nin general:\\nThis is a chat between an intelligent chatbot named Ar-\\nrowhead Expert and one human participant. The Arrowhead', metadata={'source': './copilotRC.pdf', 'page': 3}),\n",
      " Document(page_content='prompting pattern to define how the Expert shall conduct itself\\nin general:\\nThis is a chat between an intelligent chatbot named Ar-\\nrowhead Expert and one human participant. The Arrowhead\\nExpert is an expert in the Arrowhead framework and can\\naccess Arrowhead documentation and publications. The user\\nwould like to use Arrowhead for engineering (CPSoS), so help\\nthem learn and adapt the Arrowhead framework. Try to be\\nconcise with your answers, and rely only on the documentation\\nsnippets provided below. If you don’t know the answer, don’t\\nmake things up, but say so. Do not answer questions outside\\nof the Arrowhead context.\\nThis expert is able to answer reasonable questions like\\n”What are the restrictions on system naming in Arrowhead?” ,\\nor”How can manually generate certificates for a new appli-\\ncation system, if I do not have Onboarding Controller?” . It\\nwill not remember users after the session, or their chat history\\n(only up until chaining previous messages of the session can', metadata={'source': './copilotRC.pdf', 'page': 3}),\n",
      " Document(page_content='cation system, if I do not have Onboarding Controller?” . It\\nwill not remember users after the session, or their chat history\\n(only up until chaining previous messages of the session can\\nFig. 4. Overview of the Arrowhead Expert\\nfit into the prompt as context), but it is also not necessary.\\nHowever, it can provide the relevant references to the docu-\\nmentation, where the answer can be further researched. Its\\ninitial knowledge base consists of Arrowhead publications,\\nAPI and developer documentation of the core systems, and\\nthe contents of the Arrowhead Wiki, altogether 42 documents\\nfor the PoC.\\nFig. 5. Overview of the Arrowhead Management Copilot\\nMeanwhile, the Arrowhead Management Co-pilot is a more\\ncomplex assistant, as depicted in Fig. 5. It needs to (i) be able\\nto plan, (ii) remember chat context for better planning, and\\n(iii) execute API calls against the Arrowhead Core Systems.\\nThis application is deployed with Azure private resources,', metadata={'source': './copilotRC.pdf', 'page': 3}),\n",
      " Document(page_content='to plan, (ii) remember chat context for better planning, and\\n(iii) execute API calls against the Arrowhead Core Systems.\\nThis application is deployed with Azure private resources,\\nsince proper AAA is missing yet in the implementation. The\\nvector database, embedding model and blob storage are needed\\nfor storing long-term chat and document-based memory (and\\nenable RAG on top of them), while CosmosDB stores the\\nchat interactions and the plans proposed by the bot with other', metadata={'source': './copilotRC.pdf', 'page': 3}),\n",
      " Document(page_content='configuration items (like persona prompts, etc.). The solution\\nis deployed in the same private network as the Arrowhead core\\nsystems. The persona of this co-pilot is phrased as follows:\\nThis is a chat between an intelligent assistant named\\nArrowhead Co-pilot and a human participant. The Arrowhead\\nCo-pilot is an expert in Arrowhead framework and can access\\nthe Arrowhead core systems via API plugins. Try to be concise\\nwith your answers, and primarily rely on the Arrowhead\\ncore systems for providing up-to-date data and configuration\\nfrom the system of systems. If you don’t know the answer or\\ndon’t know how you can access the requested information,\\ndon’t make things up, but say so. You can also ask follow-up\\nquestions for clarification on what is required by the user.\\nThis assistant is able to answer simple questions like ”What\\nare the currently registered services in the Arrowhead Ser-\\nvice Registry?” , but it can also chain API calls together in', metadata={'source': './copilotRC.pdf', 'page': 4}),\n",
      " Document(page_content='This assistant is able to answer simple questions like ”What\\nare the currently registered services in the Arrowhead Ser-\\nvice Registry?” , but it can also chain API calls together in\\nsequential plans, if we require more complicated tasks, such\\nas”Please add an authorization rule between systems P and\\nC, for service S in the Arrowhead intra-cloud Authorization\\nSystem, but please register system X as the provider of service\\nS in the Arrowhead Service Registry first” .\\nB. Findings of the Proof of Concept\\nThese PoC applications showcase good initial impression.\\nValidation of the PoC has been done in a small workshop with\\npotential stakeholders. Both assistants can handle Arrowhead-\\ncontext and act as their appropriate personas with the intended\\nguardrails. The document RAG, long-term memory, plugin\\nhandling (i.e. making API calls to Arrowhead) and basic\\nplanner features are well received. The general UX, however,\\nis inadequate to our industrial purposes and response times are', metadata={'source': './copilotRC.pdf', 'page': 4}),\n",
      " Document(page_content='handling (i.e. making API calls to Arrowhead) and basic\\nplanner features are well received. The general UX, however,\\nis inadequate to our industrial purposes and response times are\\nconsidered long.\\nThe document RAG, with the current semantic memory\\ncapabilities, does not allow for utilizing the multi-modality\\nof (mostly IEEE-formatted) scientific publications and Arrow-\\nhead documentation. This RAG engine, while works accept-\\nably acting as long-term chat memory, is unable to work with\\ntables or diagrams of papers, which is a great loss of semantic\\ncontent. This renders the Expert unable to help with e.g. the\\nArrowhead SoS modeling concepts and visualization tasks.\\nMoreover, the somewhat ”naive” document chunking strategy\\n(i.e., every 1000 characters) is seriously limiting usability, as\\nwords are often even cut in half. Another related problem,\\nalbeit humorous, is that publications often contain (accidental)\\nhyphenations, which causes problems in understanding techni-', metadata={'source': './copilotRC.pdf', 'page': 4}),\n",
      " Document(page_content='words are often even cut in half. Another related problem,\\nalbeit humorous, is that publications often contain (accidental)\\nhyphenations, which causes problems in understanding techni-\\ncal specifications (i.e. in certificate CN names: arrowhead.eu\\nvs.arrow-head.eu ).\\nConnecting the LLMs with the Arrowhead APIs also brings\\npractical challenges. Plugins utilize openAPI3.0 specifications,\\ni.e. they work with the operationId and description of each\\nendpoint first, then with the path templates, headers, request\\nand response structures. It apparently does not bode well with\\nthe LLM, if the openAPI specification is ”too” long (even if\\nit fits the token limit) or contains a lot of seemingly similarendpoints. It can also cause unfortunate ”misunderstandings”\\nif path templates or JSON objects have depth (e.g. over 3) to\\nthem, or if the openAPI document describes them in an object-\\noriented manner (e.g. using AllOf, AnyOf directives). An', metadata={'source': './copilotRC.pdf', 'page': 4}),\n",
      " Document(page_content='if path templates or JSON objects have depth (e.g. over 3) to\\nthem, or if the openAPI document describes them in an object-\\noriented manner (e.g. using AllOf, AnyOf directives). An\\nadditional problem is when whole JSON objects or values can\\nstill be hallucinated, even if the prompt seems well grounded.\\nOpenAI has published a guideline on plugin openAPI speci-\\nfications, but this could warrant further research [25].\\nMoreover, the assistant is able to put together API calls\\nfrom different plugins as well (i.e. register services with the\\nService Registry first, then use the acquired IDs to register\\nrelated authorization rules in Authorization System) in the\\nplans. However, it is not able to handle making the logical\\nconnections between the same IDs of Arrowhead objects if\\nthose are served via different plugins. This assumes that the', metadata={'source': './copilotRC.pdf', 'page': 4}),\n",
      " Document(page_content='plans. However, it is not able to handle making the logical\\nconnections between the same IDs of Arrowhead objects if\\nthose are served via different plugins. This assumes that the\\nLLM is only able to contextualize plugins individually, and\\nhence requires plugin API designs that are self-sufficient. It is\\nalso clear that plans are static and the co-pilot has no chance\\nof making adjustments after subsequent API calls are made.\\nRegarding XAI, the current UI design allows for a minimal\\nview to the behind-the-scenes. Nevertheless, the visualization\\nof the technical prompts, plans and document citations in\\nresponses need extra consideration. Moreover, the assistant\\nshould be able to respond to follow-up questions on previous\\nplans and results, but the short-term memory cannot produce\\nwell-structured output (missing grounding for understanding\\npreviously generated internal data structures in the follow-up\\nprompt).\\nC. Future Work\\nThere is significant design, implementation and research', metadata={'source': './copilotRC.pdf', 'page': 4}),\n",
      " Document(page_content='previously generated internal data structures in the follow-up\\nprompt).\\nC. Future Work\\nThere is significant design, implementation and research\\nwork lying ahead to mature these co-pilots. The Arrow-\\nhead Expert shall be deployed with cloud-native resources,\\nbut needs to implement rate limiting and security harden-\\ning (e.g. against prompt injections) on its interactions to\\navoid anonymous jailbreaks. It also shall feature industrial-\\ngrade AAA (authentication-authorisation-accounting), while\\nintegrating into the Arrowhead PKI.\\nMeanwhile, the final implementation needs to move away\\nfrom Azure dependencies, i.e. it needs to support on-prem\\ndeployment models and enable the choice of upstream ser-\\nvices. It also needs to allow for small on-prem and cloud-\\nnative deployments, and will be implemented as an API First\\nheadless architecture.\\nOur RAG pipeline needs a complete design, where it needs\\nto be multi-modal and to process markup documents better', metadata={'source': './copilotRC.pdf', 'page': 4}),\n",
      " Document(page_content='native deployments, and will be implemented as an API First\\nheadless architecture.\\nOur RAG pipeline needs a complete design, where it needs\\nto be multi-modal and to process markup documents better\\n(Arrowhead documentation) and research publications (i.e.\\nIEEE formatted). The focus must be put on teaching the\\nExpert about the Arrowhead’s UML-based modeling paradigm\\nas well. Regarding the test automation framework for RAG,\\nwe need to establish a validation Q&A dataset with Arrowhead\\ncontext, perhaps from real-life interactions between users and\\nthe PoC. Editors of the knowledge base also need automated\\nmechanisms to be able to review the chunking process and', metadata={'source': './copilotRC.pdf', 'page': 4}),\n",
      " Document(page_content='update documents, if needed. Afterwards, the plan is to build a\\nprompt flow-based test automation framework, where answers\\nto test scenarios are to be evaluated by another customized\\nGPT, and act as a quality gate in our knowledge base-related\\nCICD (Cont. Integration and Delivery pipelines).\\nThe Expert and Management Co-pilot needs to become\\nmulti-modal as well, i.e. able to handle and use visualizations\\nin their interactions. Therefore, generating tables out of plan\\nresults, visualizing graphs and UML diagrams from the Ar-\\nrowhead runtime are on the backlog. Other XAI-related UX\\nchanges will be implemented as well, where users shall be able\\nto track and modify plans, and review and edit raw prompts\\nand API calls made by the plugins to upstream services. User\\nfeedback and telemetry collection will allow for collecting\\npotential future test scenarios. Reinforcement learning is only\\na long-term opportunity.\\nThe planner engine requires work, and should be based', metadata={'source': './copilotRC.pdf', 'page': 5}),\n",
      " Document(page_content='potential future test scenarios. Reinforcement learning is only\\na long-term opportunity.\\nThe planner engine requires work, and should be based\\non more advanced prompt engineering techniques, like Re-\\nAct [19] or interactive planner [26]. After each step, the user\\nand the LLM should have the ability to update the next API\\ncalls to be made, based on the results from previous calls.\\nThe planner engine could also utilize few-shot learning, as\\nindicated in LLM planner surveys [27].\\nFinally, the Arrowhead framework itself needs additional\\nwork as well to integrate better with co-pilots. Each core\\nservice requires additional endpoints and business logic to\\nserve compliant to the openAI plugin specifications [25].\\nMoreover, the individual API designs need to adapt and feature\\nbetter segregation of duties, so LLMs can understand available\\nmethods and their templates better.\\nIV. C ONCLUSIONS\\nThe design of LLM-based applications, like chat assistants,', metadata={'source': './copilotRC.pdf', 'page': 5}),\n",
      " Document(page_content='better segregation of duties, so LLMs can understand available\\nmethods and their templates better.\\nIV. C ONCLUSIONS\\nThe design of LLM-based applications, like chat assistants,\\nis a topic of day-by-day rapid evolution. From idea formation\\nto productizing the solution, there are no existing industrial\\nbest practices or really reusable frameworks yet. This paper\\ndescribes the first steps in the process of creating chat co-\\npilots for the CPSoS engineering domain, starting by analyzing\\nachievable capabilities offered by LLM SotA, establishing\\nfeasible use cases for the technology, creating a proof-of-\\nconcept implementation; up to specifying an R&D roadmap,\\nall in the context of the Arrowhead framework.\\nACKNOWLEDGMENT\\nProject No. KDP-IKT-2023-900-I1-00000957/0000003 has\\nbeen implemented with the support provided by the Ministry\\nof Culture and Innovation of Hungary from the National\\nResearch, Development and Innovation Fund, financed by the', metadata={'source': './copilotRC.pdf', 'page': 5}),\n",
      " Document(page_content='been implemented with the support provided by the Ministry\\nof Culture and Innovation of Hungary from the National\\nResearch, Development and Innovation Fund, financed by the\\nCo-operative Doctoral Program [C2299296] funding scheme.\\nPart of this work is created within the Arrowhead fPVN\\nproject, supported by the Chips-JU under grant agreement no.\\n101111977.REFERENCES\\n[1] OpenAI, “Chatgpt: Optimizing language models for dialogue,” https:\\n//www.openai.com/chatgpt, 2023, acc: 2023-12-28.\\n[2] C. Parnin, G. Soares, R. Pandita, S. Gulwani, J. Rich, and A. Z. Henley,\\n“Building your own product copilot: Challenges, opportunities, and\\nneeds,” 2023.\\n[3] Microsoft, “365 Copilot,” https://learn.microsoft.com/en-us/\\nmicrosoft-365-copilot/microsoft-365-copilot-overview, 2023, acc:\\n2023-12-28.\\n[4] P. J. Phillips, C. A. Hahn, P. C. Fontana, D. A. Broniatowski, and\\nM. A. Przybocki, “Four principles of explainable artificial intelligence,”\\nNISTIR, Gaithersburg, Maryland , vol. 18, 2020.', metadata={'source': './copilotRC.pdf', 'page': 5}),\n",
      " Document(page_content='[4] P. J. Phillips, C. A. Hahn, P. C. Fontana, D. A. Broniatowski, and\\nM. A. Przybocki, “Four principles of explainable artificial intelligence,”\\nNISTIR, Gaithersburg, Maryland , vol. 18, 2020.\\n[5] LangChain, “LLM Orchestration,” https://www.langchain.com/, 2023,\\nacc: 2023-12-28.\\n[6] Llamaindex, “Data framework for llm applications,” https://www.\\nllamaindex.ai/, 2023, acc: 2023-12-28.\\n[7] DeepSet AI, “Haystack,” https://github.com/deepset-ai/haystack, 2023,\\nacc: 2023-12-28.\\n[8] Microsoft, “Semantic kernel,” https://github.com/microsoft/\\nsemantic-kernel, 2023, acc: 2023-12-28.\\n[9] Eclipse Foundation, “The Arrowhead Framework,” https://projects.\\neclipse.org/projects/iot.arrowhead.\\n[10] C. Hegedus, P. Varga, and S. Tanyi, “A governance model for local and\\ninterconnecting arrowhead clouds,” 10 2020.', metadata={'source': './copilotRC.pdf', 'page': 5}),\n",
      " Document(page_content='eclipse.org/projects/iot.arrowhead.\\n[10] C. Hegedus, P. Varga, and S. Tanyi, “A governance model for local and\\ninterconnecting arrowhead clouds,” 10 2020.\\n[11] S. Pl ´osz, C. Hegedus, and P. Varga, “Advanced security considerations\\nin the arrowhead framework,” vol. 9923, 09 2016, pp. 234–245.\\n[12] Google, “Bard,” https://bard.google.com/, 08 2023.\\n[13] OWASP, “Top 10 for LLM Applications,” https://llmtop10.com/, acc:\\n2023-12-28.\\n[14] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana,\\nand S. Nanayakkara, “Improving the Domain Adaptation of Retrieval\\nAugmented Generation (RAG) Models for Open Domain Question An-\\nswering,” Transactions of the Association for Computational Linguistics ,\\nvol. 11, pp. 1–17, 01 2023.\\n[15] I. Ilin, “Advanced RAG Techniques: an Il-\\nlustrated Overview,” https://pub.towardsai.net/\\nadvanced-rag-techniques-an-illustrated-overview-04d193d8fec6.\\n[16] P. BehnamGhader, S. Miret, and S. Reddy, “Can retriever-augmented', metadata={'source': './copilotRC.pdf', 'page': 5}),\n",
      " Document(page_content='lustrated Overview,” https://pub.towardsai.net/\\nadvanced-rag-techniques-an-illustrated-overview-04d193d8fec6.\\n[16] P. BehnamGhader, S. Miret, and S. Reddy, “Can retriever-augmented\\nlanguage models reason? the blame game between the retriever and the\\nlanguage model,” 2023.\\n[17] Z. Shao, Y . Gong, Y . Shen, M. Huang, N. Duan, and W. Chen,\\n“Enhancing retrieval-augmented large language models with iterative\\nretrieval-generation synergy,” 2023.\\n[18] OpenAPI Initiative, “The openapi specification,” https://www.openapis.\\norg/, acc: 2023-12-28.\\n[19] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao,\\n“React: Synergizing reasoning and acting in language models,” 2023.\\n[20] L. Wang, W. Xu, Y . Lan, Z. Hu, Y . Lan, R. K.-W. Lee, and E.-P.\\nLim, “Plan-and-solve prompting: Improving zero-shot chain-of-thought\\nreasoning by large language models,” 2023.\\n[21] Y . Song, W. Xiong, D. Zhu, W. Wu, H. Qian, M. Song, H. Huang,', metadata={'source': './copilotRC.pdf', 'page': 5}),\n",
      " Document(page_content='Lim, “Plan-and-solve prompting: Improving zero-shot chain-of-thought\\nreasoning by large language models,” 2023.\\n[21] Y . Song, W. Xiong, D. Zhu, W. Wu, H. Qian, M. Song, H. Huang,\\nC. Li, K. Wang, R. Yao, Y . Tian, and S. Li, “Restgpt: Connecting large\\nlanguage models with real-world restful apis,” 2023.\\n[22] S. Hao, Y . Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu,\\n“Reasoning with language model is planning with world model,” 2023.\\n[23] Microsoft, “Copilot Studio,” https://www.microsoft.com/en-us/\\nmicrosoft-copilot/microsoft-copilot-studio, 2023, acc: 2023-12-28.\\n[24] G. Kulcs ´ar, P. Varga, M. S. Tatara, F. Montori, M. A. Inigo, G. Urgese,\\nand P. Azzoni, “Modeling an industrial revolution: How to manage\\nlarge-scale, complex iot ecosystems?” in 2021 IFIP/IEEE International\\nSymposium on Integrated Network Management (IM) . IEEE, 2021.\\n[25] OpenAI, “ChatGPT Plugins : Getting Started,” https://platform.openai.\\ncom/docs/plugins/getting-started, 2023, acc: 2023-12-28.', metadata={'source': './copilotRC.pdf', 'page': 5}),\n",
      " Document(page_content='Symposium on Integrated Network Management (IM) . IEEE, 2021.\\n[25] OpenAI, “ChatGPT Plugins : Getting Started,” https://platform.openai.\\ncom/docs/plugins/getting-started, 2023, acc: 2023-12-28.\\n[26] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and\\nY . Su, “Llm-planner: Few-shot grounded planning for embodied agents\\nwith large language models,” 2023.\\n[27] S. Qiao, Y . Ou, N. Zhang, X. Chen, Y . Yao, S. Deng, C. Tan, F. Huang,\\nand H. Chen, “Reasoning with language model prompting: A survey,”\\n2023.', metadata={'source': './copilotRC.pdf', 'page': 5})]\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#I am configuring chunk size to 1K, so we can see what's happening. \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(pages)\n",
    "\n",
    "pprint(splits)\n",
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to build a knowledge base using a vector database. We'll use simple in-memory vector DB. In other projects, we're using Postgres as vector DB with a plugin. \n",
    "\n",
    "Further read: https://python.langchain.com/v0.2/docs/how_to/vectorstores/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 Retrieval and generation\n",
    "\n",
    "Read: https://python.langchain.com/v0.2/docs/tutorials/rag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Fig. 3. The graphical overview of the Arrowhead Engineering Process (AEP) [24] to be supported by Co-pilots\\nthe ecosystem, answering design and integration-related\\nquestions. This can be embedded in the Arrowhead\\nFramework Wiki [9] as an inline chatbot. Intended users\\nare anyone who visits the Wiki.\\n2) The Arrowhead Management Copilot interacting with\\nvarious Arrowhead Core Systems of a Local Cloud de-\\nployment to analyze and understand, potentially manage\\nthe CPSoS via the Arrowhead governing middleware.\\nThis tool can be embedded as a widget to the Arrowhead\\nManagement Tool GUI. Intended users are the authen-\\nticated Local Cloud (SoS) operators.\\n3)Arrowhead Design Copilot which can integrate with the\\nengineering toolchain to design SoS deployment and\\nunderlying industrial automation processes and infras-\\ntructure (i.e. SysML modeling with Eclipse Papyrus).\\nThis co-pilot can be integrated into the Arrowhead\\nEngineering Toolchain, interacting with the design tools', metadata={'page': 3, 'source': './copilotRC.pdf'}),\n",
      " Document(page_content='III. C OPILOTS ACROSS THE SOS L IFECYCLE\\nA. Use Cases for Arrowhead Copilots\\nBased on this, we can establish our use cases for co-pilots\\nin the Arrowhead Engineering Process (AEP) [24] for CPSoS.\\nThe mission is the ability to design and manage a CPSoS via\\nnatural language and analyze its status and monitoring data\\nusing the potentially multi-modal output of the Copilot(s) . We\\ncan conceptualize three stages of Copilots in AEP (see Fig. 3),\\nwith increasing complexity and value-added:\\n1) An Arrowhead Expert providing a chat-based entry\\npoint into the Arrowhead architecture, feeding on the\\nalready existing documentation and publications around', metadata={'page': 2, 'source': './copilotRC.pdf'}),\n",
      " Document(page_content='tructure (i.e. SysML modeling with Eclipse Papyrus).\\nThis co-pilot can be integrated into the Arrowhead\\nEngineering Toolchain, interacting with the design tools\\nfor CPSoS. The intended users are the SoS engineers.\\nThis work demonstrates the first and second use cases,\\nand these require fairly different setups and complexity. The\\nArrowhead Expert is primarily a RAG-oriented Q&A assistant,\\nwithout the need to use long-term memory. Fig. 4 depicts the\\nRAG solution components, as demonstrator uses Microsoft\\nAzure resources for persistence, and relies on the Semantic\\nKernel middleware. Its primary knowledge base comes from\\nthe Arrowhead documentation and research papers (in IEEE\\nformat). We have also provided a few-shot training on sam-\\nple question-answers, but we primarily rely on the persona\\nprompting pattern to define how the Expert shall conduct itself\\nin general:\\nThis is a chat between an intelligent chatbot named Ar-\\nrowhead Expert and one human participant. The Arrowhead', metadata={'page': 3, 'source': './copilotRC.pdf'}),\n",
      " Document(page_content='cation system, if I do not have Onboarding Controller?” . It\\nwill not remember users after the session, or their chat history\\n(only up until chaining previous messages of the session can\\nFig. 4. Overview of the Arrowhead Expert\\nfit into the prompt as context), but it is also not necessary.\\nHowever, it can provide the relevant references to the docu-\\nmentation, where the answer can be further researched. Its\\ninitial knowledge base consists of Arrowhead publications,\\nAPI and developer documentation of the core systems, and\\nthe contents of the Arrowhead Wiki, altogether 42 documents\\nfor the PoC.\\nFig. 5. Overview of the Arrowhead Management Copilot\\nMeanwhile, the Arrowhead Management Co-pilot is a more\\ncomplex assistant, as depicted in Fig. 5. It needs to (i) be able\\nto plan, (ii) remember chat context for better planning, and\\n(iii) execute API calls against the Arrowhead Core Systems.\\nThis application is deployed with Azure private resources,', metadata={'page': 3, 'source': './copilotRC.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedder)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "pprint(retriever.invoke(\"What is arrowhead design copilot?\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IN CASE YOU NEED TO DELETE THE VECTORSTORE\n",
    "#vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating system prompt for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Use the following pieces of context to answer the question at the end.\n",
      "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "        Use three sentences maximum and keep the answer as concise as possible.\n",
      "        Always say \"thanks for asking!\" at the end of the answer.\n",
      "\n",
      "        \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
      "\n",
      "        Question: \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n",
      "\n",
      "        Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"\"\"Use the following pieces of context to answer the question at the end.\n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "        Use three sentences maximum and keep the answer as concise as possible.\n",
    "        Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Helpful Answer:\"\"\"\n",
    "    )\n",
    "])\n",
    "prompt.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    concatenated_text = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    return concatenated_text\n",
    "\n",
    "rag_chain = (\n",
    "    # creates a dictionary where context value is filled up by retriever then formatted by format_docs\n",
    "    # and question is passed over unchanged by RunnablePassthrough\n",
    "    # these are Runnable objects that will be executed in parallel or sequence and the output is fed forward\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    #prompt expects dictionary of context and question\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Arrowhead Design Copilot is a tool that integrates with the engineering toolchain to assist in designing Systems of Systems (SoS) deployment and underlying industrial automation processes. It can interact with design tools like SysML modeling with Eclipse Papyrus. The intended users are SoS engineers. Thanks for asking!'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Arrowhead Design Copilot?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further reads:\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/use_cases/question_answering/chat_history/\n",
    "\n",
    "More advanced RAG types can be better implemented using Langgraph. \n",
    "\n",
    "* Adaptive RAG\n",
    "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_adaptive_rag.ipynb \n",
    "* Corrective RAG\n",
    "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb\n",
    "* Self RAG\n",
    "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag.ipynb \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
